{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 03 â€” Self-Attention and Causal Masking\n",
        "\n",
        "## Why this day matters\n",
        "If Day 1 showed how language models predict tokens, and Day 2 showed why recurrence fails,\n",
        "Day 3 introduces the core breakthrough behind modern LLMs: **self-attention**.\n",
        "\n",
        "This notebook implements self-attention from scratch and demonstrates why **causal masking**\n",
        "is essential for autoregressive language modeling.\n",
        "\n",
        "## What is implemented\n",
        "- Single-head self-attention from first principles\n",
        "- Attention-based language model (no RNNs)\n",
        "- Visualization-ready attention weights\n",
        "- Causal masking to prevent information leakage\n",
        "\n",
        "## Key question\n",
        "Why should a model remember everything sequentially when it can directly attend to relevant tokens?\n",
        "\n"
      ],
      "metadata": {
        "id": "w6G42MOLPvUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l7szcUJNRWw",
        "outputId": "4f197e0a-5b2a-447d-81f2-3ea017631e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\n"
          ]
        }
      ],
      "source": [
        "#making raw dataset\n",
        "text = \"\"\" In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\"\"\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizations\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Charatcters:\",chars)\n",
        "print(\"Vocab Size:\",vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_3OzfiCNZ3m",
        "outputId": "4e7bd018-1956-48ab-f333-89a7b61c9f96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charatcters: [' ', '.', 'I', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
            "Vocab Size: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build mapping\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l:\"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"the\"))\n",
        "print(decode(encode(\"the\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVGxjYMrNdrc",
        "outputId": "acb34522-14e8-4037-84ea-aec6b0999628"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 11, 8]\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[:20])\n",
        "print(\"Total tokens:\",len(data))\n",
        "\n",
        "block_size = 8 # context  lenghth\n",
        "batch_size = 4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwjKv6THNpDX",
        "outputId": "16c60af1-aa4e-49ac-d1b4-eb1f942acd43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  2, 15,  0, 20, 11,  8,  0,  5,  8, 10, 12, 15, 15, 12, 15, 10,  0,\n",
            "        20, 11])\n",
            "Total tokens: 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch():\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "x,y = get_batch()\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EguiAhbuNo7T",
        "outputId": "16047cba-2332-4abb-d318-6b06c403ff7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10,  4, 18,  7,  8,  7,  0,  4],\n",
            "        [ 4,  7,  8,  0,  4,  0, 13, 16],\n",
            "        [ 7,  1,  0,  3, 11, 12, 19,  0],\n",
            "        [18, 24,  0,  4, 15, 10, 18, 24]])\n",
            "tensor([[ 4, 18,  7,  8,  7,  0,  4, 19],\n",
            "        [ 7,  8,  0,  4,  0, 13, 16, 20],\n",
            "        [ 1,  0,  3, 11, 12, 19,  0, 11],\n",
            "        [24,  0,  4, 15, 10, 18, 24,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "wvftbH-rOeb9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SINGLE-HEAD SELF-ATTENTION (FROM ZERO)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, block_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones(block_size, block_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        K = self.key(x)\n",
        "        Q = self.query(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        weights = Q @ K.transpose(-2, -1) / (C ** 0.5)\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "        out = weights @ V\n",
        "        return out, weights\n",
        "\n"
      ],
      "metadata": {
        "id": "lzNf5lfiNkXy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=64, block_size=8):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attn = SelfAttention(embed_size, block_size)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.embed(x)\n",
        "        attn_out, weights = self.attn(x)\n",
        "        logits = self.fc(attn_out)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "x7ON7dt-NEHe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionLanguageModel(vocab_size, block_size=block_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(5000):\n",
        "    xb, yb = get_batch()\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeXxnIV_Naeb",
        "outputId": "aa756cc1-3808-428b-b237-671ab925588f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 | Loss 3.2437\n",
            "Step 500 | Loss 1.2366\n",
            "Step 1000 | Loss 1.0398\n",
            "Step 1500 | Loss 0.9294\n",
            "Step 2000 | Loss 0.8060\n",
            "Step 2500 | Loss 0.9295\n",
            "Step 3000 | Loss 0.8717\n",
            "Step 3500 | Loss 1.0302\n",
            "Step 4000 | Loss 0.5897\n",
            "Step 4500 | Loss 0.6894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_attn(model, start_char, max_new_tokens=100):\n",
        "    idx = torch.tensor([[stoi[start_char]]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop idx to the last block_size tokens if its length exceeds block_size\n",
        "        # This ensures that the input to the model does not exceed the context window\n",
        "        block_size_attn = model.attn.tril.size(0) # Get the block_size from the attention module\n",
        "        idx_cond = idx if idx.size(1) <= block_size_attn else idx[:, -block_size_attn:]\n",
        "\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_idx = torch.multinomial(probs, 1)\n",
        "        idx = torch.cat([idx, next_idx], dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(generate_attn(model, 'I'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsy9SZqwNihw",
        "outputId": "4b9db794-236f-47e6-9361-d24777c12589"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ininng t.t.T his hade ve ungrarded as a was was creas bangd win t.Thide haden begind win this wis mad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "\n",
        "- Attention produces more stable training than RNNs\n",
        "- Model can access the full context simultaneously\n",
        "- Without causal masking, repetition and degeneration occur\n",
        "- With masking, text quality improves significantly\n",
        "\n",
        "## Key Insight (Day 3)\n",
        "Self-attention replaces memory with **dynamic relevance**.\n",
        "Instead of remembering, the model looks back and decides what matters.\n"
      ],
      "metadata": {
        "id": "bkTVsEY5QJcb"
      }
    }
  ]
}