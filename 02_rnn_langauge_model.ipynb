{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "as we had added the rnn , so for understanding why memory helps, i had break down this code in three major step which are : 1) replacing bigram with rnn, 2)comparing the output and 3) stress test memory"
      ],
      "metadata": {
        "id": "YYLcAoFI6KFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making raw dataset\n",
        "text = \"\"\" In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\"\"\"\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4KQwx7O5BVT",
        "outputId": "d821a021-411d-4bde-c6db-95f66e908ef9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizations\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Charatcters:\",chars)\n",
        "print(\"Vocab Size:\",vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsMtLRS25Cb3",
        "outputId": "d5072a81-bc52-4756-c5e2-768478588f34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charatcters: [' ', '.', 'I', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
            "Vocab Size: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build mapping\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l:\"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"the\"))\n",
        "print(decode(encode(\"the\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2JJ61LP5G_z",
        "outputId": "d4ca5ab9-8045-4cc1-fc52-871851f650e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 11, 8]\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[:20])\n",
        "print(\"Total tokens:\",len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6yCa4Qj5NK3",
        "outputId": "9950f230-3bbd-4dba-ea8e-dbd6b0606588"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  2, 15,  0, 20, 11,  8,  0,  5,  8, 10, 12, 15, 15, 12, 15, 10,  0,\n",
            "        20, 11])\n",
            "Total tokens: 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context  lenghth\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch():\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "x,y = get_batch()\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DeODFzo5TxT",
        "outputId": "92ac6ef4-f13d-446e-e46b-0fa5754e8ca4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[16,  9,  0, 17,  8, 16, 17, 13],\n",
            "        [ 8, 18, 24,  0,  4, 15, 10, 18],\n",
            "        [ 2, 15,  0, 20, 11,  8,  0,  5],\n",
            "        [10,  4, 18,  7,  8,  7,  0,  4]])\n",
            "tensor([[ 9,  0, 17,  8, 16, 17, 13,  8],\n",
            "        [18, 24,  0,  4, 15, 10, 18, 24],\n",
            "        [15,  0, 20, 11,  8,  0,  5,  8],\n",
            "        [ 4, 18,  7,  8,  7,  0,  4, 19]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WxqjX56Kuw3v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN MODEL\n",
        "class RNNLangaugeModel(nn.Module):\n",
        "  def __init__ (self,vocab_size,embed_size=32, hidden_size=64):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size,embed_size)\n",
        "    self.rnn = nn.RNN(embed_size,hidden_size,batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    x =self.embed(x)\n",
        "    out, _ = self.rnn(x)\n",
        "    logits = self.fc(out)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n"
      ],
      "metadata": {
        "id": "uQ631McUvIq_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "model = RNNLangaugeModel(vocab_size=100)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "for step in range(5000):\n",
        "  xb, yb = get_batch()\n",
        "  logits, loss = model(xb,yb)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(f\"Step{step}| Loss{loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYP9L-REzyYo",
        "outputId": "f05c8f16-0d32-405b-a7e5-785ff1692c63"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step0| Loss4.6678\n",
            "Step500| Loss0.4885\n",
            "Step1000| Loss0.2386\n",
            "Step1500| Loss0.3234\n",
            "Step2000| Loss0.3738\n",
            "Step2500| Loss0.3523\n",
            "Step3000| Loss0.3009\n",
            "Step3500| Loss0.2663\n",
            "Step4000| Loss0.1725\n",
            "Step4500| Loss0.2671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#genrat text\n",
        "def generate_rnn(model,start_char,max_new_tokens = 100):\n",
        "  idx = torch.tensor([[stoi[start_char]]])\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits, _ = model(idx)\n",
        "    logits =logits[:,-1,:]\n",
        "    probs = F.softmax (logits, dim=-1)\n",
        "    next_idx = torch.multinomial(probs,1)\n",
        "    idx = torch.cat([idx,next_idx],dim=1)\n",
        "\n",
        "  return decode(idx[0].tolist())\n",
        "print(generate_rnn(model,'I'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzHbAewU0QT6",
        "outputId": "34d2ebcd-9431-4b53-efc7-ce5b4c9a4d3f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the beginning the universe was created. This has made a lot of people very angry This has made a l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the core concept i learn from this lesson is that , Memory helps, but squential processing is a dead end at scale"
      ],
      "metadata": {
        "id": "fFpVWbsk7kHT"
      }
    }
  ]
}