# LLM-FROM-SCRATCH
Learning Large Language Models from raw text to agentic Systems - implemented from scratch with experiments.

## progress log

###01 Day 01
-impplemented a character-level language model from strach 
-Built custom tokenization and context-based batching
-Trained a simple neural network to genrate  text

###02 Day 02
-Replaced bigram model with an RNN-based language model
-Modeled sequential dependencies using hidden states
-Observed training instability and memory limitaions of vanilla RNNs

### Day 03
- Implemented self-attention from scratch
- Built an attention-based language model
- Demonstrated failure without causal masking
- Implemented GPT-style masked attention


### Day 04
- Implemented multi-head masked self-attention
- Added learned positional embeddings
- Built a complete Transformer block
- Assembled a mini Transformer language model


