{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4j5IAfqTteM",
        "outputId": "11a58d17-2f0b-4d69-adc2-69ce813d9e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\n"
          ]
        }
      ],
      "source": [
        "#making raw dataset\n",
        "text = \"\"\" In the beginning the universe was created. This has made a lot of people very angry and been widely regarded as bad move\"\"\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizations\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Charatcters:\",chars)\n",
        "print(\"Vocab Size:\",vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whMIKjLoUg6u",
        "outputId": "d9da5daf-0e36-4244-cc94-608c9ddbd049"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charatcters: [' ', '.', 'I', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
            "Vocab Size: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build mapping\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l:\"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"the\"))\n",
        "print(decode(encode(\"the\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rMR1NdBVv63",
        "outputId": "9c6d800d-06c0-42e4-b90c-6c041a295019"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 11, 8]\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[:20])\n",
        "print(\"Total tokens:\",len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqBTRlouYE2i",
        "outputId": "1ed60f73-453b-40ec-ff21-e047f709e996"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  2, 15,  0, 20, 11,  8,  0,  5,  8, 10, 12, 15, 15, 12, 15, 10,  0,\n",
            "        20, 11])\n",
            "Total tokens: 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context  lenghth\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch():\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size]for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "x,y = get_batch()\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDJr4NttYr57",
        "outputId": "764bfbbf-f0bb-4f4c-dd11-bf26d24f1a31"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[18, 19,  8,  0, 23,  4, 19,  0],\n",
            "        [10, 12, 15, 15, 12, 15, 10,  0],\n",
            "        [ 7,  8,  7,  0,  4, 19,  0,  5],\n",
            "        [ 2, 15,  0, 20, 11,  8,  0,  5]])\n",
            "tensor([[19,  8,  0, 23,  4, 19,  0,  6],\n",
            "        [12, 15, 15, 12, 15, 10,  0, 20],\n",
            "        [ 8,  7,  0,  4, 19,  0,  5,  4],\n",
            "        [15,  0, 20, 11,  8,  0,  5,  8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple neural network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self,idx, targets=None):\n",
        "    logits = self.token_embedding(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "G0bDtBe3dZll"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "model = BigramModel(vocab_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(3000):\n",
        "  xb,yb = get_batch()\n",
        "  logits, loss = model(xb,yb)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(f\"Step{step} | Loss{loss.item():.4f}\")\n",
        "\n",
        "  # loss downward = model .learning probabilityv patterns\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAP7jjRAfmxz",
        "outputId": "12814557-8722-4775-f80b-c04f965a7c6c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step0 | Loss3.3010\n",
            "Step500 | Loss3.1720\n",
            "Step1000 | Loss2.7727\n",
            "Step1500 | Loss2.7569\n",
            "Step2000 | Loss2.5024\n",
            "Step2500 | Loss1.9821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#genrate text\n",
        "def generate(model,start_char, max_new_tokens = 100):\n",
        "  idx = torch.tensor([[stoi[start_char]]])\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits,_ = model(idx)\n",
        "    logits = logits[:,-1,:]\n",
        "    probs = F.softmax(logits,dim =-1)\n",
        "    next_idx = torch.multinomial(probs,num_samples = 1)\n",
        "    idx = torch.cat([idx,next_idx],dim =1)\n",
        "  return decode(idx[0].tolist())\n",
        "\n",
        "print(generate(model,\"I\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBmxPuPJhdBX",
        "outputId": "fb7c0784-1081-4b02-9446-e6055424a7b0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inininders arydrpe arfglyive as tearIvenrs heobofb. g hTthenderde cre barsas wlvpsmot Ivas a IveeggTh\n"
          ]
        }
      ]
    }
  ]
}